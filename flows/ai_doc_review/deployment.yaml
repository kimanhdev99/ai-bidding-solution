$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: ai-doc-review-deployment
endpoint_name: VAR_ENDPOINT_NAME
model: azureml:ai-doc-review-model:VAR_MODEL_VERSION
environment:
  build:
    path: .
    dockerfile_path: Dockerfile
  # inference config is used to build a serving container for online deployments
  inference_config:
    liveness_route:
      path: /health
      port: 8080
    readiness_route:
      path: /health
      port: 8080
    scoring_route:
      path: /score
      port: 8080
instance_type: Standard_DS3_v2
instance_count: 1
request_settings:
  # 5 min timeout
  request_timeout_ms: 300000
  max_concurrent_requests_per_instance: 10
environment_variables:
  # for pulling connections from workspace
  PRT_CONFIG_OVERRIDE: deployment.subscription_id=VAR_SUBSCRIPTION_ID,deployment.resource_group=VAR_RESOURCE_GROUP,deployment.workspace_name=VAR_AI_HUB_PROJECT_NAME,deployment.endpoint_name=VAR_ENDPOINT_NAME,deployment.deployment_name=ai-doc-review-deployment
  AZURE_CLIENT_ID: VAR_IDENTITY_CLIENT_ID
  # (Optional) When there are multiple fields in the response, using this env variable will filter the fields to expose in the response.
  # For example, if there are 2 flow outputs: "answer", "context", and I only want to have "answer" in the endpoint response, I can set this env variable to '["answer"]'.
  # If you don't set this environment, by default all flow outputs will be included in the endpoint response.
  PROMPTFLOW_RESPONSE_INCLUDED_FIELDS: '["flow_output_streaming"]'
  DOCUMENT_INTELLIGENCE_ENDPOINT: VAR_DOCUMENT_INTELLIGENCE_ENDPOINT
  STORAGE_URL_PREFIX: VAR_STORAGE_URL_PREFIX
  AZURE_OPENAI_ENDPOINT: VAR_AZURE_OPENAI_ENDPOINT
